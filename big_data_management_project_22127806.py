# -*- coding: utf-8 -*-
"""Big Data Management Project - 22127806.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eE8MYGYyOhGRlMbSGx5bvhCK8zU5smBX

# **Asad Waheed - 22127806 - BIG Data Management Final Project** 
# Catch the Pink Flamingo
"""

#Here we combines file uploading functionality with displaying system paths and the Python executable, which is helpful for various purposes such as inspecting the system environment, managing dependencies, or working with local files in Google Colab

from google.colab import files
uploaded = files.upload()

import sys
import os

sys.path
sys.executable

!apt-get install openjdk-8-jdk-headless

!wget https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop2.7.tgz

!tar xf /content/spark-3.2.1-bin-hadoop2.7.tgz

!pip install pyspark

!pip install -q findspark

import os

os.environ["JAVA_HOME"] =  "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.2.1-bin-hadoop2.7"


import findspark

findspark.init()
findspark.find()

import pyspark
sparkcontext = pyspark.SparkContext(master="local[*]", appName="BDM_Project")

sparkcontext.setLogLevel("ERROR")

from pyspark.sql import SparkSession

flamingo = SparkSession.builder.master("local").appName("bigdatamanagement_project_22127806").config("spark.ui.port", "4050").getOrCreate()

flamingo

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("BDM_Project_22127806").getOrCreate()

buyclicks_df = spark.read.csv("/content/buy-clicks.csv", header=True, inferSchema=True)

buyclicks_df.printSchema()

buyclicks_df.show(10)

buyclicks_df.describe().toPandas().transpose()

buyclicks_df.describe().show()

"""# **DATA EXPLORATION**"""

#loading essential libraries
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.sql.types import StringType, StructField, StructType

# Defining the file paths for each dataset
file_paths = {
    "ad-clicks.csv": "/content/ad-clicks.csv",
    "buy-clicks.csv": "/content/buy-clicks.csv",
    "game-clicks.csv": "/content/game-clicks.csv",
    "level-events.csv": "/content/level-events.csv",
    "team-assignments.csv": "/content/team-assignments.csv",
    "team.csv": "/content/team.csv",
    "user-session.csv": "/content/user-session.csv",
    "users.csv": "/content/users.csv",
    "all-csv": "/content/all.csv"
}

# Creating an empty list to store the schema and data types information
data = []

# Iterating through each dataset and extract the schema and data types
for file_name, file_path in file_paths.items():
    # Reading the dataset into a DataFrame
    df = spark.read.csv(file_path, header=True, inferSchema=True)
    
    # Getting the schema and data types
    schema = df.schema
    data_types = [(field.name, field.dataType) for field in schema.fields]
    
    # Appending the dataset schema and data types to the list
    data.extend([(file_name, name, str(data_type)) for name, data_type in data_types])

# Defining the schema for the schema_df DataFrame
schema = StructType([
    StructField("Dataset", StringType(), nullable=False),
    StructField("Column", StringType(), nullable=False),
    StructField("Data_Type", StringType(), nullable=False)
])

# Creating a DataFrame with the schema and data types information
schema_df = spark.createDataFrame(data, schema=schema)

# Showing the final schema and data types DataFrame
schema_df.show(schema_df.count(), truncate=False)

# Creating an empty list to store the dataset information
recordcounts_info = []

# Iterating through each dataset and count the number of records
for file_name, file_path in file_paths.items():
    # Reading the dataset into a DataFrame
    df = spark.read.csv(file_path, header=True, inferSchema=True)
    
    # Counting the number of records
    record_count = df.count()
    
    # Appending the dataset information to the list
    recordcounts_info.append((file_name, record_count))

# Creating a DataFrame from the list of dataset information
schema = ["Dataset", "Record_Count"]
dataset_df = spark.createDataFrame(recordcounts_info, schema)

# Showing the dataset DataFrame
dataset_df.show()

import matplotlib.pyplot as plt
import numpy as np
from matplotlib.colors import LinearSegmentedColormap

# Dataset and Record_Count values
data = [
    ("ad-clicks.csv", 16323),
    ("buy-clicks.csv", 2947),
    ("game-clicks.csv", 755806),
    ("level-events.csv", 1254),
    ("team-assignments.csv", 9826),
    ("team.csv", 109),
    ("user-session.csv", 9250),
    ("users.csv", 2393)
]

# Extract the labels and counts
labels = [row[0] for row in data]
counts = [row[1] for row in data]

# Rearrange color values based on counts
sorted_data = sorted(data, key=lambda x: x[1])
sorted_counts = [row[1] for row in sorted_data]

# Create a customized colormap with blue and green gradients
colors = ["#A4C2F4", "#7EAEEF", "#5A9BEA", "#3687E5", "#1274E0", "#0F66C6", "#0D58AC", "#0A4A92"]
cmap = LinearSegmentedColormap.from_list("my_cmap", colors)

# Create the horizontal bar chart with gradient colors
plt.barh(range(len(sorted_data)), np.log10(sorted_counts), color=cmap(np.linspace(0, 1, len(sorted_data))))

# Add the values on the bars
for i, count in enumerate(sorted_counts):
    plt.text(np.log10(count), i, str(count), va='center', color='black')

# Customize the chart
plt.yticks(range(len(sorted_data)), [row[0] for row in sorted_data])
plt.xlabel('Record Count (log scale)')
plt.ylabel('Dataset')
plt.title('Number of Records in Each Dataset')

# Adjust the spacing
plt.xlim(0, np.log10(max(counts)) + 0.9)
plt.ylim(-0.5, len(data) - 0.5)

# Display the chart
plt.show()

import pandas as pd

# Dataset filenames
filenames = [
    "ad-clicks.csv",
    "buy-clicks.csv",
    "game-clicks.csv",
    "level-events.csv",
    "team-assignments.csv",
    "team.csv",
    "user-session.csv",
    "users.csv"
]

# Load and handle missing values for each dataset
for filename in filenames:
    # Load dataset into a DataFrame
    df = pd.read_csv(filename)
    
    # Check for missing values
    missing_values = df.isnull().sum()
    print("Missing Values in", filename, ":\n", missing_values)
    
    # Handle missing values
    # Option 1: Drop rows with missing values
    df_dropped = df.dropna()
    
    # Option 2: Fill missing values with a specific value
    df_filled = df.fillna('N/A')
    
    # Option 3: Fill missing values with mean, median, or mode
    df_mean = df.fillna(df.mean())
    df_median = df.fillna(df.median())
    df_mode = df.fillna(df.mode().iloc[0])
    
    # Print the head of the modified DataFrames
    print("\nDropped DataFrame:\n", df_dropped.head())
    print("\nFilled DataFrame:\n", df_filled.head())
    print("\nMean-filled DataFrame:\n", df_mean.head())
    print("\nMedian-filled DataFrame:\n", df_median.head())
    print("\nMode-filled DataFrame:\n", df_mode.head())
    print("-------------------------------------------")

# Initialize lists to store dataset names and missing value counts
dataset_names = []
missing_value_counts = []

# Load and count missing values for each dataset
for filename in filenames:
    # Load dataset into a DataFrame
    df = pd.read_csv(filename)
    
    # Count missing values
    missing_values = df.isnull().sum().sum()
    
    # Add dataset name and missing value count to lists
    dataset_names.append(filename)
    missing_value_counts.append(missing_values)

# Create a bar chart for missing value counts
plt.bar(dataset_names, missing_value_counts)

# Add labels and title
plt.xlabel('Dataset')
plt.ylabel('Missing Value Count')
plt.title('Number of Missing Values in Each Dataset')

# Rotate x-axis labels for better readability
plt.xticks(rotation=90)

# Display the chart
plt.show()

# Define the list of dataset names
dataset_names = [
    "ad-clicks.csv",
    "buy-clicks.csv",
    "game-clicks.csv",
    "level-events.csv",
    "team-assignments.csv",
    "team.csv",
    "user-session.csv",
    "users.csv"
]

# Iterate through each dataset
for dataset_name in dataset_names:
    # Load the dataset into a DataFrame
    df = spark.read.csv(dataset_name, header=True, inferSchema=True)

    # Identify duplicate records
    duplicate_rows = df.groupBy(df.columns).count().filter(col("count") > 1)

    # Count the number of duplicate records
    duplicate_count = duplicate_rows.count()

    # Print the duplicate records and count
    print(f"Duplicate Records in {dataset_name} ({duplicate_count} duplicates):")
    duplicate_rows.show(truncate=False)

# Dataset names
dataset_names = [
    "ad-clicks.csv",
    "buy-clicks.csv",
    "game-clicks.csv",
    "level-events.csv",
    "team-assignments.csv",
    "team.csv",
    "user-session.csv",
    "users.csv"
]

# Iterate through each dataset
for dataset_name in dataset_names:
    # Load the dataset into a DataFrame
    df = spark.read.csv(dataset_name, header=True, inferSchema=True)

    # List of categorical columns to explore
    categorical_columns = df.columns

    # Iterate through each categorical column
    for column in categorical_columns:
        # Count the occurrences of each category
        category_counts = df.groupBy(column).count().orderBy(col("count").desc())

        # Show the distribution of categories
        print(f"Distribution of {column} in {dataset_name}:")
        category_counts.show(truncate=False)
        print()

users = spark.read.csv("/content/users.csv", header =True, inferSchema=True)

from pyspark.sql import functions as F
from pyspark.sql.types import IntegerType

# Calculate age from DOB
current_date = F.current_date()
users_age_group_df = users.withColumn("age", F.year(current_date) - F.year("dob"))

# Adjust age for cases where the current month is before the birth month
users_age_group_df = users_age_group_df.withColumn("birth_month", F.month("dob"))
users_age_group_df = users_age_group_df.withColumn("current_month", F.month(current_date))
users_age_group_df = users_age_group_df.withColumn("age", F.when(F.col("current_month") < F.col("birth_month"), F.col("age") - 1).otherwise(F.col("age")))

# Convert age column to IntegerType
users_age_group_df = users_age_group_df.withColumn("age", users_age_group_df["age"].cast(IntegerType()))

# Define age ranges and corresponding labels for age groups
age_ranges = [(0, 17), (18, 25), (26, 35), (36, 50), (51, 65), (66, 100)]
age_labels = ["0-17", "18-25", "26-35", "36-50", "51-65", "66+"]

# Convert age into age group
users_age_group_df = users_age_group_df.withColumn("age_group", F.when(F.col("age").isNull(), None))
for i in range(len(age_ranges)):
    min_age, max_age = age_ranges[i]
    age_label = age_labels[i]
    users_age_group_df = users_age_group_df.withColumn("age_group", 
                                                       F.when((F.col("age") >= min_age) & (F.col("age") <= max_age),
                                                              age_label).otherwise(F.col("age_group")))

users_age_group_df.show(10)

users_age_group_df.toPandas()[["age"]].describe().transpose()

age_group_counts_df = users_age_group_df.groupBy("age_group").count()

age_group_counts_df.show()

"""The analysis of age group distribution in the "Catch the Pink Flamingo" game dataset reveals several key insights. Firstly, the game demonstrates a strong user base within the age groups of 26-35, 36-50, and 51-65, indicating broad appeal across a wide range of adult players. This finding highlights the game's ability to attract and engage players in their prime earning and spending years. Additionally, these age groups exhibit higher counts, suggesting active participation and potential for long-term user retention. The game's popularity among mid-aged and older individuals presents an opportunity for targeted marketing strategies, enabling developers to tailor promotional campaigns to specific age groups. By understanding the age group dynamics, developers can optimize gameplay experiences and create personalized features that resonate with the preferences and interests of different player segments. These insights not only enhance the game's overall success but also provide valuable guidance for future game development and marketing efforts in the competitive mobile gaming industry."""

import matplotlib.pyplot as plt

# Extract age group labels and counts
age_groups = [row['age_group'] for row in age_group_counts_df.collect()]
counts = [row['count'] for row in age_group_counts_df.collect()]

# Set color scheme
color = '#329D9C'

# Create bar plot
plt.bar(age_groups, counts, color=color)

# Customize the plot
plt.xlabel('Age Group')
plt.ylabel('Count')
plt.title('Players Count by Age Group')
plt.xticks(rotation=45)

# Display the plot
plt.show()

from pyspark.sql.functions import count

# Select the age_group and country columns
age_group_country_df = users_age_group_df.select("age_group", "country")

# Group by age_group and country, and count the number of users
user_count_df = age_group_country_df.groupBy("age_group", "country").agg(count("*").alias("user_count"))

# Sort the results by age group
user_count_df = user_count_df.orderBy("age_group")

# Display the DataFrame
user_count_df.show()

"""# **Hide**"""

import matplotlib.pyplot as plt
import pandas as pd

# Convert user_count_df to Pandas DataFrame
df = user_count_df.toPandas()

# Sort the DataFrame by user_count in descending order
df_sorted = df.sort_values('user_count', ascending=False)

# Select the top 20 rows
df_top20 = df_sorted.head(20)

# Extract the age_group, country, and user_count columns
age_groups = df_top20['age_group'].tolist()
countries = df_top20['country'].tolist()
user_counts = df_top20['user_count'].tolist()

# Set the size of the markers based on user_count
marker_sizes = [count * 10 for count in user_counts]

# Plot the scatter plot
plt.figure(figsize=(10, 6))
plt.scatter(age_groups, countries, s=marker_sizes, alpha=0.5)
plt.xlabel('Age Group')
plt.ylabel('Country')
plt.title('Top 20 User Counts by Age Group and Country')
plt.xticks(rotation=45)
plt.grid(True)

# Show the plot
plt.show()

"""# **Cońtińet**"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Read the country mapping CSV file into a DataFrame
country_mapping_df = spark.read.csv("/content/all.csv", header=True)

# Join users DataFrame with country mapping DataFrame on country code
users_with_country_df = users_age_group_df.join(country_mapping_df, users_age_group_df["country"] == country_mapping_df["alpha-2"], "left")

# Select the necessary columns: country name and continent
users_with_country_df = users_with_country_df.select(col("userId"), col("nick"), col("dob"), col("name").alias("country_name"), col("region").alias("continent"))

# Show the resulting DataFrame
users_with_country_df.show(10)

column_names = country_mapping_df.columns
print(column_names)

!pip install squarify

from pyspark.sql.functions import col, sum
from pyspark.sql.window import Window
import plotly.express as px

# Join user_count_df with country_mapping_df on country code
df = user_count_df.join(country_mapping_df, user_count_df["country"] == country_mapping_df["alpha-2"], "left")

# Select the necessary columns: age_group, country, user_count, country_name, continent
df = df.select(user_count_df["age_group"], user_count_df["country"], user_count_df["user_count"],
               country_mapping_df["name"].alias("country_name"), country_mapping_df["region"].alias("continent"))

# Filter out any null values
df = df.na.drop()

# Calculate the sum of user_count per continent
window_spec = Window.partitionBy("continent")
df = df.withColumn("continent_total", sum("user_count").over(window_spec))

# Calculate the percentage of users in each continent
df = df.withColumn("percent", col("user_count") / col("continent_total") * 100)

# Convert DataFrame to Pandas for plotting
pandas_df = df.toPandas()

# Plot the treemap using Plotly
fig = px.treemap(pandas_df, path=['continent', 'country_name'], values='percent',
                 color='percent', color_continuous_scale='Blues', hover_data=['user_count'])

fig.update_layout(title='Treemap of user distribution by continent', uniformtext=dict(minsize=10, mode='hide'))

fig.show()

from pyspark.sql.functions import col

# Join user_count_df with country_mapping_df on country code
df = user_count_df.join(country_mapping_df, user_count_df["country"] == country_mapping_df["alpha-2"], "left")

# Select the necessary columns: age_group, country, user_count, country_name, continent
df = df.select(user_count_df["age_group"], user_count_df["country"], user_count_df["user_count"],
               country_mapping_df["name"].alias("country_name"), country_mapping_df["region"].alias("continent"))

# Filter out any null values
df = df.na.drop()

# Convert DataFrame to Pandas for treemap visualization
pandas_df = df.toPandas()

# Calculate the percentage of users in each continent
pandas_df['Percent'] = pandas_df['user_count'] / pandas_df['user_count'].sum() * 100

# Plot the treemap using pandas and squarify
import matplotlib.pyplot as plt
import squarify

# Group the data by continent and calculate the sum of percentages
continent_data = pandas_df.groupby('continent')['Percent'].sum()

# Plot the treemap
plt.figure(figsize=(16, 8), dpi=80)
colors = plt.cm.Set3(range(len(continent_data)))
squarify.plot(sizes=continent_data, label=continent_data.index, color=colors, alpha=0.8,
              text_kwargs={'horizontalalignment': 'center', 'verticalalignment': 'center', 'fontsize': 12, 'fontweight': 'bold'})
plt.title('Treemap of user distribution by continent')
plt.axis('off')
plt.show()





from pyspark.sql import SparkSession

# Read the users.csv file
users_df = spark.read.csv("/content/users.csv", header=True, inferSchema=True)

# Read the buy-clicks.csv file
buy_clicks_df = spark.read.csv("/content/buy-clicks.csv", header=True, inferSchema=True)

# Create temporary views for both DataFrames
users_df.createOrReplaceTempView("users")
buy_clicks_df.createOrReplaceTempView("buy_clicks")

# Join the two DataFrames using Spark SQL
joined_df = spark.sql("""
    SELECT u.*, b.timestamp, b.txId, b.userSessionId, b.team, b.buyId, b.price
    FROM users u
    INNER JOIN buy_clicks b ON u.userId = b.userId
""")

# Show the joined DataFrame
joined_df.show()

from pyspark.sql import SparkSession
from pyspark.sql.functions import sum

# Create a SparkSession
spark = SparkSession.builder.getOrCreate()

# Read the users.csv file
users_df = spark.read.csv("/content/users.csv", header=True, inferSchema=True)

# Read the buy-clicks.csv file
buy_clicks_df = spark.read.csv("/content/buy-clicks.csv", header=True, inferSchema=True)

# Create temporary views for both DataFrames
users_df.createOrReplaceTempView("users")
buy_clicks_df.createOrReplaceTempView("buy_clicks")

# Read the country_mapping.csv file
country_mapping_df = spark.read.csv("/content/all.csv", header=True)

# Create a temporary view for the country mapping DataFrame
country_mapping_df.createOrReplaceTempView("country_mapping")

# Join the two DataFrames using Spark SQL
joined_df = spark.sql("""
    SELECT u.*, b.timestamp, b.txId, b.userSessionId, b.team, b.buyId, b.price, cm.region as continent
    FROM users u
    INNER JOIN buy_clicks b ON u.userId = b.userId
    LEFT JOIN country_mapping cm ON u.country = cm.`alpha-2`
""")

# Calculate the total spending by continent
continent_spending = joined_df.groupBy("continent").agg(sum("price").alias("total_spending"))

# Sort the spending in descending order
sorted_spending = continent_spending.orderBy("total_spending", ascending=False)

# Display the highest spending continents
sorted_spending.show()

import matplotlib.pyplot as plt
import pandas as pd

# Convert the sorted_spending DataFrame to Pandas for chart visualization
pandas_df = sorted_spending.toPandas()

# Remove the "null" continent from the DataFrame
pandas_df = pandas_df[pandas_df['continent'].notnull()]

# Set the figure size
fig, ax = plt.subplots(figsize=(11, 8))

# Create a pie chart
colors = ['#00b8a9', '#ff6600', '#ffcc00', '#82b300', '#9ed1de']  # Specify custom colors
explode = [0.1] * len(pandas_df['total_spending'])  # Add explosion effect
labels = [f'{p} ({c})' for p, c in zip(pandas_df['continent'], pandas_df['total_spending'])]  # Include count in labels

# Create the pie chart
wedges, texts, autotexts = ax.pie(pandas_df['total_spending'], labels=labels, colors=colors,
                                  explode=explode, autopct='%1.1f%%', startangle=90)

# Add percentage values inside each wedge
for autotext in autotexts:
    autotext.set_color('white')
    autotext.set_fontweight('bold')

# Set plot title
ax.set_title("Total Spending by Continent")

# Equal aspect ratio ensures that the pie is drawn as a circle
ax.axis('equal')

# Show the plot
plt.show()

from pyspark.sql.types import StringType

# Get the list of column names in the DataFrame
columns = df.columns

# Initialize an empty list to store the categorical variables
categorical_variables = []

# Iterate through each column
for column in columns:
    # Check the data type of the column
    data_type = df.schema[column].dataType
    
    # Check if the data type is string
    if isinstance(data_type, StringType):
        categorical_variables.append(column)

# Print the identified categorical variables
print("Categorical Variables:")
for variable in categorical_variables:
    print("- " + variable)

from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Create a Spark session
spark = SparkSession.builder.getOrCreate()

# Define the list of dataset names
dataset_names = ["ad-clicks.csv", "buy-clicks.csv", "game-clicks.csv", "level-events.csv", "team-assignments.csv", "team.csv", "user-session.csv", "users.csv"]

# Iterate over the datasets and identify numerical columns
for dataset_name in dataset_names:
    # Read the dataset into a DataFrame
    dataset_df = spark.read.csv(dataset_name, header=True, inferSchema=True)

    # Get the data types of all columns
    column_types = dataset_df.dtypes

    # Identify the numerical columns
    numerical_columns = [column[0] for column in column_types if column[1] in ['int', 'bigint', 'float', 'double']]

    # Print the dataset name and numerical columns
    print("Dataset:", dataset_name)
    print("Numerical Columns:")
    for column in numerical_columns:
        print(column)
    print()

# Dataset names
dataset_names = [
    "ad-clicks.csv",
    "buy-clicks.csv",
    "game-clicks.csv",
    "level-events.csv",
    "team-assignments.csv",
    "team.csv",
    "user-session.csv",
    "users.csv"
]

# Iterate through each dataset
for dataset_name in dataset_names:
    # Load the dataset into a DataFrame
    df = spark.read.csv(dataset_name, header=True, inferSchema=True)

    # Filter the numerical columns
    numerical_columns = [column for column in df.columns if isinstance(df.schema[column].dataType, (IntegerType, DoubleType))]

    # Calculate the summary statistics for numerical columns
    summary_stats = df.select(numerical_columns).describe().toPandas().transpose()

    # Print the summary statistics for the dataset
    print(f"Summary statistics for {dataset_name}:")
    print(summary_stats)
    print()

from pyspark.sql.types import IntegerType, DoubleType
from pyspark.sql import SparkSession
from pyspark.sql.types import IntegerType, DoubleType
import matplotlib.pyplot as plt
import pandas as pd
import re

# Create a SparkSession
spark = SparkSession.builder.getOrCreate()

# Load the "ad-clicks.csv" dataset into a DataFrame
dataset_name = "ad-clicks.csv"
df = spark.read.csv(dataset_name, header=True, inferSchema=True)

# Filter the categorical columns excluding timestamp column
categorical_columns = [column for column in df.columns
                       if not isinstance(df.schema[column].dataType, (IntegerType, DoubleType))
                       and column.lower() != "timestamp"]

# Visualize categorical columns using count plots
for column in categorical_columns:
    counts = df.groupBy(column).count().toPandas()
    counts.plot(kind='bar', x=column, y='count', legend=False)
    plt.title(f"Distribution of {column} in {dataset_name}")
    plt.xlabel(column)
    plt.ylabel("Count")
    plt.tight_layout()
    plt.show()

# Load the "team-assignments.csv" dataset into a DataFrame
dataset_name = "user-session.csv"
df = spark.read.csv(dataset_name, header=True, inferSchema=True)

# Filter the categorical columns excluding timestamp and teamCreationTime columns
categorical_columns = [column for column in df.columns
                       if not isinstance(df.schema[column].dataType, (IntegerType, DoubleType))
                       and column.lower() != "timestamp"
                       and column.lower() != "teamcreationtime"]

# Visualize categorical columns using count plots
for column in categorical_columns:
    counts = df.groupBy(column).count().toPandas()
    counts.plot(kind='bar', x=column, y='count', legend=False)
    plt.title(f"Distribution of {column} in {dataset_name}")
    plt.xlabel(column)
    plt.ylabel("Count")
    plt.tight_layout()
    plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from pyspark.sql import SparkSession
import pyspark.sql.functions as F

# Create a SparkSession
spark = SparkSession.builder.appName("PieChartExample").getOrCreate()

# Read the CSV file into a DataFrame
user_session_df = spark.read.csv("/content/user-session.csv", header=True, inferSchema=True)

# Group by platformType and calculate the total count
platform_usage_stats_df = user_session_df.groupBy("platformType").agg(
    F.count("platformType").alias("total_platform_count")
).orderBy(F.desc("total_platform_count"))

# Convert the Spark DataFrame to a Pandas DataFrame
data = platform_usage_stats_df.toPandas()

# Prepare data for plotting
platform_types = data["platformType"]
total_platform_count = data["total_platform_count"]

# Create a 2D pie chart
fig, ax = plt.subplots(figsize=(11, 8))

# Create the pie chart
colors = ['#00b8a9', '#ff6600', '#ffcc00', '#82b300', '#9ed1de']  # Specify custom colors
explode = [0.1] * len(total_platform_count)  # Add explosion effect
labels = [f'{p} ({c})' for p, c in zip(platform_types, total_platform_count)]  # Include count in labels

# Set bold and italic properties for labels
label_props = {'style': 'italic'}
ax.pie(total_platform_count, labels=labels, colors=colors, explode=explode,
       autopct='%1.1f%%', shadow=True, startangle=90, textprops=label_props)

# Set aspect ratio to 'equal' to ensure a circular pie chart
ax.axis('equal')

# Set title with increased titlepad value
ax.set_title('Most Frequently Used Platforms', pad=30)

# Show the plot
plt.show()

from pyspark.sql import SparkSession
import pyspark.sql.functions as F
import matplotlib.pyplot as plt

# Create a SparkSession
spark = SparkSession.builder.appName("AdClicksAnalysis").getOrCreate()

# Load the "ad-clicks.csv" dataset into a DataFrame
dataset_name = "/content/ad-clicks.csv"
df = spark.read.csv(dataset_name, header=True, inferSchema=True)

# Group by adCategory and calculate the click count
ad_category_stats_df = df.groupBy("adCategory").agg(
    F.count("adCategory").alias("click_count")
).orderBy(F.desc("click_count"))

# Convert the Spark DataFrame to a Pandas DataFrame for visualization
data = ad_category_stats_df.toPandas()

# Plot the ad category distribution
plt.figure(figsize=(10, 6))
plt.bar(data["adCategory"], data["click_count"], color='#82b300')

plt.title("Distribution of Ad Categories")
plt.xlabel("Ad Category")
plt.ylabel("Click Count")
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

"""
The insights provided by the results are as follows:

Distribution of ad categories: The result shows the count of each ad category, indicating the frequency of ads belonging to each category. For example, the "computers" category has the highest count of 2638, followed by "games" with 2601.

Click-through rates (CTR) for different ad categories: The CTR represents the percentage of clicks received by each ad category out of the total clicks. The result shows the CTR for each ad category, indicating the effectiveness of ads in generating clicks. For example, the "computers" category has a CTR of 16.16%, indicating a relatively higher click-through rate compared to other categories.

Most clicked ads and their corresponding categories: The result shows the top 10 most clicked ads along with their corresponding ad categories and the count of clicks they received. This provides insights into which specific ads are attracting the most user engagement. For example, adId 20 and adId 24 belong to the "clothing" category and have received 609 and 604 clicks, respectively.

These insights help understand the distribution of ad categories, the effectiveness of different categories in generating clicks, and the specific ads that are performing well in terms of click counts. This information can be valuable for ad campaign optimization, targeting specific categories or ads that are more likely to attract user engagement."""

from pyspark.sql import SparkSession
import pyspark.sql.functions as F

# Create a SparkSession
spark = SparkSession.builder.appName("AdClicksAnalysis").getOrCreate()

# Load the "ad-clicks.csv" dataset into a DataFrame
dataset_name = "ad-clicks.csv"
df = spark.read.csv(dataset_name, header=True, inferSchema=True)

# Distribution of ad categories (adCategory) and their frequencies
ad_category_distribution = df.groupBy("adCategory").count().orderBy(F.desc("count"))

# Click-through rates (CTR) for different ad categories
total_clicks = df.count()
ctr = df.groupBy("adCategory").agg((F.count("adCategory") / total_clicks * 100).alias("CTR")).orderBy(F.desc("CTR"))

# Most clicked ads (adId) and their corresponding categories
most_clicked_ads = df.groupBy("adId", "adCategory").count().orderBy(F.desc("count")).limit(10)

# Show the results
ad_category_distribution.show()
ctr.show()
most_clicked_ads.show()

"""Click-through rates (CTR) for different ad categories: The CTR represents the percentage of clicks received by each ad category out of the total clicks. The result shows the CTR for each ad category, indicating the effectiveness of ads in generating clicks. For example, the "computers" category has a CTR of 16.16%, indicating a relatively higher click-through rate compared to other categories."""

import pandas as pd
import matplotlib.pyplot as plt

ctr_data = pd.DataFrame({
    'adCategory': ['computers', 'games', 'clothing', 'sports', 'fashion', 'movies', 'hardware', 'electronics', 'automotive'],
    'CTR': [16.16, 15.93, 14.34, 12.71, 10.58, 10.37, 9.73, 6.72, 3.47]
})

# Plotting the click-through rates (CTR) for different ad categories
plt.figure(figsize=(10, 6))
plt.bar(ctr_data['adCategory'], ctr_data['CTR'], color='#00b7d8')
plt.xlabel('Ad Category')
plt.ylabel('CTR (%)')
plt.title('Click-Through Rates (CTR) by Ad Category')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

from pyspark.sql import SparkSession
from pyspark.sql.functions import hour, dayofweek

# Create a SparkSession
spark = SparkSession.builder.getOrCreate()

# Load the "ad-clicks.csv" dataset into a DataFrame
dataset_name = "ad-clicks.csv"
df = spark.read.csv(dataset_name, header=True, inferSchema=True)

# Extract hour of the day from timestamp
df = df.withColumn("hour_of_day", hour(df["timestamp"]))

# Extract day of the week from timestamp
df = df.withColumn("day_of_week", dayofweek(df["timestamp"]))

# Analyze the distribution of clicks over time
hourly_clicks = df.groupBy("hour_of_day").count().orderBy("hour_of_day")
daily_clicks = df.groupBy("day_of_week").count().orderBy("day_of_week")

# Show the hourly click distribution
print("Hourly Click Distribution:")
hourly_clicks.show()

# Show the daily click distribution
print("Daily Click Distribution:")
daily_clicks.show()

# Perform additional analysis or visualization as per your requirements
# ...

"""Hourly Click Distribution:

The highest number of clicks occurs during the 9th hour of the day (9 AM), with 799 clicks.
Clicks remain relatively consistent throughout the day, with minor fluctuations.
The lowest number of clicks occurs during the 0th hour of the day (12 AM), with 605 clicks.
Daily Click Distribution:

Days 1 to 5 (Monday to Friday) have a relatively consistent number of clicks, ranging from 2405 to 2503.
Day 6 (Saturday) has a lower number of clicks compared to weekdays, with 1891 clicks.
Day 7 (Sunday) shows a slight increase in clicks compared to Saturday, with 2150 clicks.
These insights provide an overview of the click distribution over hours of the day and days of the week. They can be further analyzed to identify patterns, peak times, or specific trends that may influence ad click behavior.
"""

import matplotlib.pyplot as plt
import calendar

# Convert the Spark DataFrame to Pandas DataFrame
hourly_clicks_pd = hourly_clicks.toPandas()
daily_clicks_pd = daily_clicks.toPandas()

# Plot the hourly click distribution
plt.figure(figsize=(10, 6))
plt.plot(hourly_clicks_pd["hour_of_day"], hourly_clicks_pd["count"], marker="o")
plt.title("Hourly Click Distribution")
plt.xlabel("Hour of the Day")
plt.ylabel("Click Count")
plt.grid(True)
plt.show()

# Plot the daily click distribution
plt.figure(figsize=(10, 6))
day_of_week_names = [calendar.day_name[i - 1] for i in daily_clicks_pd["day_of_week"]]
plt.plot(day_of_week_names, daily_clicks_pd["count"], marker="o")
plt.title("Daily Click Distribution")
plt.xlabel("Day of the Week")
plt.ylabel("Click Count")
plt.grid(True)
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Load the buy-clicks.csv file into a DataFrame
buy_clicks_df = pd.read_csv('buy-clicks.csv')

# Count the frequency of each item (buyId)
item_counts = buy_clicks_df['buyId'].value_counts().sort_index()

# Create a bar plot
plt.figure(figsize=(10, 6))
plt.bar(item_counts.index, item_counts.values, color='#FE7A15', edgecolor='black')

# Add value labels on top of each bar
for i, count in enumerate(item_counts.values):
    plt.text(i, count, str(count), ha='center', va='bottom', fontweight='bold')

plt.xlabel('Item ID (buyId)')
plt.ylabel('Frequency')
plt.title('Number of Purchases per Item')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Load the buy-clicks.csv file into a DataFrame
buy_clicks_df = pd.read_csv('buy-clicks.csv')

# Calculate the revenue for each item (buyId)
revenue_per_item = buy_clicks_df.groupby('buyId')['price'].sum() * 0.02

# Create a bar plot
plt.figure(figsize=(10, 6))
plt.bar(revenue_per_item.index, revenue_per_item.values, color='#FE676E', edgecolor='black')

# Add value labels on top of each bar
for i, revenue in enumerate(revenue_per_item.values):
    plt.text(i, revenue, f'${revenue:.2f}', ha='center', va='bottom', fontweight='bold')

plt.xlabel('Item ID (buyId)')
plt.ylabel('Revenue')
plt.title('Revenue Generated per Item')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()