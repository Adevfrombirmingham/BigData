# -*- coding: utf-8 -*-
"""BigData ML - 22127806.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iFHNIzjiHcwCSNjuef9ioh1_6F0JLz95

# **PySpark Setup**
"""

!apt-get install openjdk-8-jdk-headless

!pip install pyspark

import os

os.environ["JAVA_HOME"] =  "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.2.1-bin-hadoop2.7"


import findspark

findspark.init()
findspark.find()

!wget https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop2.7.tgz

!tar xf /content/spark-3.2.1-bin-hadoop2.7.tgz

!pip install -q findspark

import pyspark
sparkcontext = pyspark.SparkContext(master="local[*]", appName="BD_ML_22127806_Project")

sparkcontext.setLogLevel("ERROR")

import pyspark
import numpy as np
import pandas as pd

from pyspark.sql import SparkSession

SPARK = SparkSession.builder\
        .master("local")\
        .appName("Big_Data_Assessment")\
        .config('spark.ui.port', '4050')\
        .getOrCreate()

#printing spark variable

SPARK

"""# **Data Preprocessing - Combined Chat Dataset**"""

#libraries
from pyspark.sql.functions import col, when
from pyspark.sql.functions import sum as f_sum
from pyspark.ml.feature import StringIndexer
from pyspark.ml.feature import VectorAssembler

combined_df = SPARK.read.csv("/content/combined-data.csv", header=True, inferSchema=True)
combined_df.show(10)
combined_df.count()

##calculating the count of rows in the DataFrame combined_df after dropping any rows that contain missing values.
combined_df.na.drop().count()

df = combined_df.fillna(0)
df.show(10)
df.count()

combined_df = combined_df.filter(col("avg_price") != "NULL")

# Creating a new column HighSpender_LowSpender based on avg price
updated_df = combined_df.withColumn("HighSpender_LowSpender", when(col("avg_price") > 5.0, 1).otherwise(0))

# Count the occurrences of High Spenders and Low Spenders
high_spenders = updated_df.filter(col("HighSpender_LowSpender") == "1").count()
low_spenders = updated_df.filter(col("HighSpender_LowSpender") == "0").count()

print("High Spenders:", high_spenders)
print("Low Spenders:", low_spenders)

updated_df.show(5)

# Select the desired columns
selected_df = updated_df.select(
    "userId",
    "teamLevel",
    "userSessionId",
    "platformType",
    "count_gameclicks",
    "count_hits",
    "count_buyId",
    "HighSpender_LowSpender"
)

selected_df.show(5)

transform_df = updated_df.groupby("userId") \
    .agg(f_sum("count_gameclicks").alias('total_gameclicks'),
         f_sum("count_hits").alias("total_hits"))
    
# Join DataFrames
Join_df = updated_df.join(transform_df, "userId", "inner")
Join_df.show()

updated_df.printSchema()

# Creating a StringIndexer instance
string_indexer = StringIndexer(inputCol="platformType", outputCol="platformTypeIndex")
Index_modeling = string_indexer.fit(Join_df)
# Fitting the StringIndexer to the data and transform the DataFrame
indexed_df = string_indexer.fit(Join_df).transform(Join_df)

# Showing the converted DataFrame
indexed_df.show(5)

Classification_df = indexed_df.select('teamLevel','total_hits','platformTypeIndex','HighSpender_LowSpender')

Classification_df.show(5)

"""# **Vectorization**"""

# Creating a VectorAssembler object
VectorAssembler_df = VectorAssembler(
    inputCols=['total_hits', 'teamLevel', 'platformTypeIndex'],
    outputCol='features'
)

# Consolidating predictor columns
assembled_df = VectorAssembler_df.transform(Classification_df)

# Showing the assembled DataFrame
assembled_df.show(5)

"""# **Train Test Model**

The purpose of the code is to split the assembled_df DataFrame into a training set and a testing set in an 80:20 ratio. The training set will be used to train a machine learning model, while the testing set will be used to evaluate the performance of the trained model on unseen data.

The randomSplit method is used to perform the split, where the first argument [0.8, 0.2] represents the proportions of the training set and testing set, respectively. The second argument seed=22127806 is optional and sets a specific random seed to ensure reproducibility of the split.

The code then calculates the percentage of records in the training set compared to the total number of records in the assembled_df. It prints the value of training, which represents the ratio of the training set size to the total dataset size.

The output will be a floating-point value between 0 and 1, representing the proportion of records in the training set. If the output is approximately 0.8, it indicates that the training set contains around 80% of the records in the original dataset.
"""

# Spliting into training and test sets in a 80:20 ratio
training_set, testing_set = assembled_df.randomSplit([0.8, 0.2], seed=22127806)

# Checking that training set has around 80% of records
training = training_set.count() / assembled_df.count()
print(training)

"""# **Logistic Regression**"""

from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator

# Create a logistic regression model
lr = LogisticRegression(labelCol="HighSpender_LowSpender", featuresCol="features")

# Train the model on the training set
model = lr.fit(training_set)

# Make predictions on the testing set
predictions = model.transform(testing_set)

# Evaluate the model
evaluator = BinaryClassificationEvaluator(rawPredictionCol="rawPrediction", labelCol="HighSpender_LowSpender")
accuracy = evaluator.evaluate(predictions)

# Print the accuracy
print("Accuracy:", accuracy)

# Print the model coefficients and intercept
coefficients = model.coefficients
intercept = model.intercept
print("Coefficients:", coefficients)
print("Intercept:", intercept)

from pyspark.ml.functions import vector_to_array
from pyspark.sql.functions import expr

# Convert raw prediction vector to an array of values
predictions = predictions.withColumn('rawPredictionArray', vector_to_array('rawPrediction'))

# Assign predicted labels based on raw predictions
predictions = predictions.withColumn('prediction_label', expr('CASE WHEN rawPredictionArray[1] >= 0.5 THEN 1.0 ELSE 0.0 END'))

# Calculate the confusion matrix
confusion_matrix = predictions.groupBy('HighSpender_LowSpender', 'prediction_label').count()
confusion_matrix.show()

from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator

# Calculate true positives (TP)
TP = predictions.filter("HighSpender_LowSpender = 1 AND prediction_label = 1.0").count()

# Calculate false positives (FP)
FP = predictions.filter("HighSpender_LowSpender = 0 AND prediction_label = 1.0").count()

# Calculate false negatives (FN)
FN = predictions.filter("HighSpender_LowSpender = 1 AND prediction_label = 0.0").count()

# Calculate precision and recall
precision = TP / (TP + FP)
recall = TP / (TP + FN)
print('precision = {:.2f}\nrecall   = {:.2f}'.format(precision, recall))

# Find weighted precision
multi_evaluator = MulticlassClassificationEvaluator().setLabelCol("HighSpender_LowSpender")
weighted_precision = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: "weightedPrecision"})

# Find AUC
binary_evaluator = BinaryClassificationEvaluator().setLabelCol("HighSpender_LowSpender")
auc = binary_evaluator.evaluate(predictions, {binary_evaluator.metricName: "areaUnderROC"})

print("Area Under the Curve:",auc)
print("Weighted_Precision:", weighted_precision)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, confusion_matrix

# Convert labels to integers
labels = np.array(predictions.select("HighSpender_LowSpender").rdd.map(lambda x: int(x.HighSpender_LowSpender)).collect())

# Convert probabilities to a NumPy array
probs = np.array(predictions.select("prediction_label").rdd.map(lambda x: float(x.prediction_label)).collect())

# Calculate ROC curve
fpr, tpr, _ = roc_curve(labels, probs)

# Calculate AUC
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

# Define the confusion matrix values
conf_matrix = np.array([[162, 16], [25, 111]])  # Replace with your actual values

# Set the labels for the confusion matrix
labels = ['LowSpender', 'HighSpender']

sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues')

# Add labels, title, and axis ticks
plt.xlabel('Predictions')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.xticks([0.5, 1.5], ['Negative','Positive'])
plt.yticks([0.5, 1.5], ['Negative','Positive'])
plt.show()

"""# **Decision Tree Algorithm**"""

from pyspark.ml.classification import DecisionTreeClassifier
from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

# Train the Decision Tree classifier
dt = DecisionTreeClassifier(labelCol="HighSpender_LowSpender", featuresCol="features")
dt_model = dt.fit(training_set)

# Make predictions on the test data
dt_predictions = dt_model.transform(testing_set)

# Calculate true positives (TP)
TP = dt_predictions.filter("HighSpender_LowSpender = 1 AND prediction = 1.0").count()

# Calculate false positives (FP)
FP = dt_predictions.filter("HighSpender_LowSpender = 0 AND prediction = 1.0").count()

# Calculate false negatives (FN)
FN = dt_predictions.filter("HighSpender_LowSpender = 1 AND prediction = 0.0").count()

# Calculate true negatives (TN)
TN = dt_predictions.filter("HighSpender_LowSpender = 0 AND prediction = 0.0").count()

#calculating ACCURACY
accuracy = (TP + TN) / (TP + TN + FP + FN)
print('Accuracy: {:.2f}'.format(accuracy))

# Calculate precision and recall
precision = TP / (TP + FP)
recall = TP / (TP + FN)
print('Precision = {:.2f}\nRecall   = {:.2f}'.format(precision, recall))

# Find weighted precision
multi_evaluator = MulticlassClassificationEvaluator().setLabelCol("HighSpender_LowSpender")
weighted_precision = multi_evaluator.evaluate(dt_predictions, {multi_evaluator.metricName: "weightedPrecision"})

# Find AUC
binary_evaluator = BinaryClassificationEvaluator().setLabelCol("HighSpender_LowSpender")
auc = binary_evaluator.evaluate(dt_predictions, {binary_evaluator.metricName: "areaUnderROC"})

print("Area Under the Curve:", auc)
print("Weighted Precision:", weighted_precision)

# Define the confusion matrix values
conf_matrix = np.array([[TN, FP], [FN, TP]])

# Set the labels for the confusion matrix
labels = ['Low-Spender', 'High Spender']

sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Reds')
# Add labels, title, and axis ticks
plt.xlabel('Predictions')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.xticks([0.5, 1.5], ['Negative', 'Positive'])
plt.yticks([0.5, 1.5], ['Negative', 'Positive'])
plt.show()

from pyspark.ml.evaluation import BinaryClassificationEvaluator
import matplotlib.pyplot as plt

# Evaluate model using BinaryClassificationEvaluator
evaluator = BinaryClassificationEvaluator().setLabelCol("HighSpender_LowSpender")
roc = evaluator.setMetricName("areaUnderROC").evaluate(predictions)

# Plot ROC curve
plt.plot([0, 1], [0, 1], 'r--')
plt.plot(fpr, tpr, label='ROC Curve (AUC = {:.2f})'.format(roc))
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Receiver Operating Characteristic (ROC)")
plt.legend(loc="lower right")
plt.show()

# Print AUC
print("AUC:", roc)

from pyspark.sql.functions import expr

# Calculate true positives (TP)
TP = dt_predictions.filter("HighSpender_LowSpender = 1 AND prediction = 1.0").count()

# Calculate false positives (FP)
FP = dt_predictions.filter("HighSpender_LowSpender = 0 AND prediction = 1.0").count()

# Calculate false negatives (FN)
FN = dt_predictions.filter("HighSpender_LowSpender = 1 AND prediction = 0.0").count()

# Calculate true negatives (TN)
TN = dt_predictions.filter("HighSpender_LowSpender = 0 AND prediction = 0.0").count()

# Create the confusion matrix DataFrame
confusion_matrix = SPARK.createDataFrame([
    (1, 0.0, TP),
    (0, 0.0, FP),
    (1, 1.0, FN),
    (0, 1.0, TN)
], ["HighSpender_LowSpender", "prediction_label", "count"])

# Display the confusion matrix table
confusion_matrix.show()

"""# **Clustering Algorithms**
1 - K Mean Clustering
2 - DBSCAN
3 - Heirarichal 
"""

indexed_df.show(5)

combined_df.show(5)

import pyspark.sql.functions as f
from pyspark.sql import Window
from pyspark.ml.feature import StringIndexer

# Window specification
window = Window.rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)

# Indexing
index = StringIndexer(inputCol='platformType', outputCol='platformType_indexed')
index_model = index.fit(combined_df)
index_df = index_model.transform(combined_df)

# Replacing "NULL" values in avg_price column with 0
index_df = index_df.withColumn("avg_price", f.when(f.col("avg_price") == "NULL", 0).otherwise(f.col("avg_price")))

# Calculating total sum of avg_price using window function
index_df = index_df.withColumn('total', f.sum(f.col('avg_price')).over(window))

# Creating Spender_NonSpender column based on avg_price threshold
index_df = index_df.withColumn("HighSpender_LowSpender", f.when(f.col("avg_price") > 5, 1).otherwise(0))

index_df.show(5)

# Grouping by userId and calculate total_gameclicks, total_hits, avg_price_total
clustering_df = index_df.groupby("userId") \
    .agg(f.sum("count_gameclicks").alias('total_gameclicks'),
         f.sum("count_hits").alias("total_hits"),
         f.sum("avg_price").alias("avg_price_total")) \
    .withColumn('Percent', f.col('total_hits') * 100 / f.col('total_gameclicks')) \
    .withColumn("HighHitter_LowHitter", f.when(f.col("Percent") > 10, 1).otherwise(0))

clustering_df.show(5)

# Joining cluster_df and index_df on userId
kmean_algo_df = clustering_df.join(index_df, "userId", "inner")

kmean_algo_df.show(5)

import pyspark.ml.feature as ft
from pyspark.ml.clustering import KMeans
from pyspark.ml import Pipeline

# Defining the features to be used in the model
featuresUsed = [
    'teamLevel',
    'avg_price_total',
    'total_hits',
    'platformType_indexed',
    'HighHitter_LowHitter',
    'HighSpender_LowSpender'
]

# Creating a VectorAssembler to assemble the features into a single vector column
assemble = ft.VectorAssembler(inputCols=featuresUsed, outputCol='features')

# Creating a StandardScaler to standardize the features
scaling = ft.StandardScaler(inputCol='features', outputCol='standardized')

# Creating a KMeans model with the desired number of clusters
kmeansalgo = KMeans(featuresCol='standardized')

# Assembling the pipeline with the defined stages
pipeline = Pipeline(stages=[assemble, scaling, kmeansalgo])

import pyspark.ml.feature as ft
from pyspark.ml.clustering import KMeans
from pyspark.ml import Pipeline

# Defineing the features to be used in the model
featuresUsed = [
    'teamLevel',
    'avg_price_total',
    'total_hits',
    'platformType_indexed',
    'HighHitter_LowHitter',
    'HighSpender_LowSpender'
]

# Dropping the existing 'features' column if it already exists
if 'features' in kmean_algo_df.columns:
    kmean_algo_df = kmean_algo_df.drop('features')

# Creating a VectorAssembler to assemble the features into a single vector column
assemble = ft.VectorAssembler(inputCols=featuresUsed, outputCol='features')

# Creating a StandardScaler to standardize the features
scaling = ft.StandardScaler(inputCol='features', outputCol='standardized')

# Creating a KMeans model with the desired number of clusters
kmeansalgo = KMeans(featuresCol='standardized')

# Assembling the pipeline with the defined stages
pipeline = Pipeline(stages=[assemble, scaling, kmeansalgo])

import ssl
from pyspark.ml.evaluation import ClusteringEvaluator

# Set up evaluator 
evaluator = ClusteringEvaluator()

# HYPERPARAMETER TUNING between k=2 and 15
for k in range(2, 15):
    # Setting the KMeans stage of the pipeline to hold each value of K and the random seed = 2212780
    KM = pipeline.getStages()[-1].setK(k).setSeed(22127806)
    KM_MODEL = pipeline.fit(kmean_algo_df)

    # Building a predictions dataset for each k value
    predictionKM = KM_MODEL.transform(kmean_algo_df)

    # Silhouette score each prediction set and print formatted output
    ss = evaluator.evaluate(predictionKM)
    print(f'Tested: {k} clusters: {ss}')

pipeline.getStages()[-1].setK(7).setSeed(2212780)  # set the random seed for the algorithm and the value for k

# Fitting the model and transform the data
modelKM = pipeline.fit(kmean_algo_df)
clustersKM = modelKM.transform(kmean_algo_df)
clustersKM.show()

# Printing cluster centers
kmeansalgo_model = modelKM.stages[-1]
KMcluster_centers = kmeansalgo_model.clusterCenters()
for i, center in enumerate(KMcluster_centers):
    print(f"Cluster {i}: {center}")

import plotly.express as px
KM_df = clustersKM.toPandas()

# building figure with 3D numeric dimensions and categorical isPlanet and prediction dimensions
KMfig = px.scatter_3d(KM_df, x='avg_price_total', y='total_hits', z='teamLevel', color='prediction', template='ggplot2')
KMfig.show()

print(KM_df)

import plotly.express as px

KM_df = clustersKM.toPandas()

# Building figure with 2D numeric dimensions and categorical prediction dimension
Kmeanfig = px.scatter(KM_df, x='avg_price_total', y='total_hits', color='prediction', template='ggplot2')
Kmeanfig.show()

import matplotlib.pyplot as plt

# Extract the predicted cluster labels from the predictions DataFrame
cluster_labels = predictions.select('prediction').rdd.flatMap(lambda x: x).collect()

# Extract the feature vectors from the predictions DataFrame
features = predictions.select('features').rdd.flatMap(lambda x: x).collect()

# Convert the feature vectors to a list of lists
feature_values = [list(vec) for vec in features]

# Extract the coordinates for the first two features
x = [values[0] for values in feature_values]
y = [values[1] for values in feature_values]

# Plot the clusters using different colors for each cluster
plt.scatter(x, y, c=cluster_labels)
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('KMeans Clustering')
plt.show()

"""# **Heirarichal Clustering Model**

"""

from pyspark.ml.clustering import BisectingKMeans
from pyspark.ml.evaluation import ClusteringEvaluator
from pyspark.ml.feature import VectorAssembler

# Select the features you want to use for clustering
featuresUsed = ['teamLevel',
    'avg_price_total',
    'total_hits',
    'platformType_indexed',
    'HighHitter_LowHitter',
    'HighSpender_LowSpender']  # Replace with actual column names

assembler = VectorAssembler(inputCols=featuresUsed, outputCol="features")
assembled_data = assembler.transform(kmean_algo_df)

# Drop the existing "prediction" column if it exists
if "prediction" in assembled_data.columns:
    assembled_data = assembled_data.drop("prediction")

# Perform hierarchical clustering with different numbers of clusters
silhouette_scores = []
evaluator = ClusteringEvaluator(featuresCol="features", metricName="silhouette")
for K in range(2, 15):
    bkmeans = BisectingKMeans(featuresCol="features", k=K, minDivisibleClusterSize=1)
    bkmeans_fit = bkmeans.fit(assembled_data)
    bkmeans_transform = bkmeans_fit.transform(assembled_data)
    evaluation_score = evaluator.evaluate(bkmeans_transform)
    silhouette_scores.append(evaluation_score)

# Print the silhouette scores for different numbers of clusters
for K, score in zip(range(2, 15), silhouette_scores):
    print(f"Silhouette score for K={K}: {score}")

import matplotlib.pyplot as plt

# Plotting the silhouette scores
plt.plot(range(2, 15), silhouette_scores, marker='o', linestyle='-', color='b')

# Set labels and title
plt.xlabel("Number of Clusters (K)")
plt.ylabel("Silhouette Score")
plt.title("Silhouette Score for Different Numbers of Clusters")

# Add gridlines
plt.grid(True, linestyle='--', alpha=0.5)

# Customize tick marks
plt.xticks(range(2, 15))
plt.yticks([i/10 for i in range(11)])

# Set plot background color
plt.gca().set_facecolor('#f2f2f2')

# Set border color and width
plt.gca().spines['bottom'].set_color('gray')
plt.gca().spines['bottom'].set_linewidth(0.5)
plt.gca().spines['left'].set_color('gray')
plt.gca().spines['left'].set_linewidth(0.5)

# Add a legend
plt.legend(['Silhouette Scores'], loc='upper right')

# Adjust plot margins
plt.margins(0.05)

# Display the plot
plt.show()

!pip install matplotlib

from pyspark.ml.feature import PCA
import matplotlib.pyplot as plt

bkmean=BisectingKMeans(featuresCol='features', k=4) 
bkmean_Model=bkmean.fit(assembled_data)
bkmean_transform=bkmean_Model.transform(assembled_data)

#performing the Principal Component Analysis
from pyspark.ml.feature import PCA as PCAml
import numpy as np
Analysis = PCAml(k=2, inputCol="features", outputCol="pca")
Analysis_model = Analysis.fit(assembled_data)
Analysis_Transformation = Analysis_model.transform(assembled_data)

Extraction = Analysis_Transformation.rdd.map(lambda row: row.pca).collect()
Extraction = np.array(Extraction)
ClusterAssigning = np.array(bkmean_transform.rdd.map(lambda row: row.prediction).collect()).reshape(-1,1)
ClusterAssigning

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Combining PCA components with cluster assignments
Component_Analysis = np.hstack((Extraction, ClusterAssigning))
Component_Analysis_df = pd.DataFrame(data=Component_Analysis, columns=("PCA 1", "PCA 2", "ClusterAssigning"))

# Creating a scatter plot with color-coded clusters
sns.scatterplot(data=Component_Analysis_df, x="PCA 1", y="PCA 2", hue="ClusterAssigning", palette="viridis")

plt.xlabel("PCA 1")
plt.ylabel("PCA 2")
plt.title("PCA Visualization with Cluster Assignments")
plt.show()